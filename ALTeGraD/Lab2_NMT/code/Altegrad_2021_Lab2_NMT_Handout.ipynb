{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCJvlnvsKALE"
   },
   "source": [
    "<center><h2>ALTeGraD 2021<br>Lab Session 2: NMT</h2><h3> Neural Machine Translation</h3> 16 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DB6pvLvlKbtD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqIFlSfYTwk8"
   },
   "source": [
    "## Define the Encoder / Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Kc8cQTFkKmif"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    to be passed the entire source sequence at once\n",
    "    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n",
    "    https://pytorch.org/docs/stable/nn.html#gru\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # fill the gaps # (transform input into embeddings and pass embeddings to RNN)\n",
    "        # you should return a tensor of shape (seq, batch, feat)\n",
    "        embedded = self.embedding(input)\n",
    "        hs, h_n = self.rnn(embedded)\n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bn9iO9wNT2p7"
   },
   "source": [
    "## Define the Attention layer / Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JwUAUDL4KmoM"
   },
   "outputs": [],
   "source": [
    "class seq2seqAtt(nn.Module):\n",
    "    '''\n",
    "    concat global attention a la Luong et al. 2015 (subsection 3.1)\n",
    "    https://arxiv.org/pdf/1508.04025.pdf\n",
    "    '''\n",
    "    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n",
    "        super(seq2seqAtt, self).__init__()\n",
    "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t, hidden_dim)\n",
    "        self.ff_score = nn.Linear(hidden_dim, 1, bias=False) # just a dot product here\n",
    "    \n",
    "    def forward(self, target_h, source_hs):\n",
    "        target_h_rep = target_h.repeat(source_hs.size(0), 1, 1) # (1, batch, feat) -> (seq, batch, feat)\n",
    "        # fill the gaps #\n",
    "        # implement the score computation part of the concat formulation (see section 3.1. of Luong 2015)\n",
    "        concat_output = torch.cat([target_h_rep, source_hs], dim=-1)\n",
    "        scores = self.ff_score(torch.tanh(self.ff_concat(concat_output))) # should be of shape (seq, batch, 1)\n",
    "        scores = scores.squeeze(dim=2) # (seq, batch, 1) -> (seq, batch). dim = 2 because we don't want to squeeze the batch dim if batch size = 1\n",
    "        norm_scores = torch.softmax(scores, dim=0)\n",
    "        source_hs_p = source_hs.permute((2, 0, 1)) # (seq, batch, feat) -> (feat, seq, batch)\n",
    "        weighted_source_hs = (norm_scores * source_hs_p) # (seq, batch) * (feat, seq, batch) (* checks from right to left that the dimensions match)\n",
    "        ct = torch.sum(weighted_source_hs.permute((1, 2, 0)), 0, keepdim=True) # (feat, seq, batch) -> (seq, batch, feat) -> (1, batch, feat); keepdim otherwise sum squeezes \n",
    "        return ct, norm_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNnGEa5cT9ka"
   },
   "source": [
    "## Define the Decoder layer / Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h7tLaq4PK90q"
   },
   "outputs": [],
   "source": [
    "# import torch.nn.functionnal as F\n",
    "class Decoder(nn.Module):\n",
    "    '''to be used one timestep at a time\n",
    "       see https://pytorch.org/docs/stable/nn.html#gru'''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim_s, hidden_dim, padding_idx):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim, hidden_dim)\n",
    "        self.predict = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, input, source_context, h):\n",
    "        # fill the gaps #\n",
    "        # transform input into embeddings, pass embeddings to RNN, concatenate with source_context and apply tanh, and make the prediction\n",
    "        # prediction should be of shape (1, batch, vocab), h and tilde_h of shape (1, batch, feat)\n",
    "        embedded = self.embedding(input)\n",
    "        hs, h = self.rnn(embedded, h)\n",
    "        tilde_h = torch.tanh( self.ff_concat(torch.cat([source_context, h], dim=-1)) )\n",
    "        prediction = torch.softmax( self.predict(tilde_h), dim=-1)\n",
    "        return prediction, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUT6D3JETX8H"
   },
   "source": [
    "# Define the full seq2seq model / Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FYX0K3dNK-c9"
   },
   "outputs": [],
   "source": [
    "class seq2seqModel(nn.Module):\n",
    "    '''the full seq2seq model'''\n",
    "    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n",
    "     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n",
    "     'oov_token','sos_token','eos_token','max_size']\n",
    "    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t, \n",
    "                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n",
    "                 oov_token, sos_token, eos_token, max_size):\n",
    "        super(seq2seqModel, self).__init__()\n",
    "        self.vocab_s = vocab_s\n",
    "        self.source_language = source_language\n",
    "        self.vocab_t_inv = vocab_t_inv\n",
    "        self.embedding_dim_s = embedding_dim_s\n",
    "        self.embedding_dim_t = embedding_dim_t\n",
    "        self.hidden_dim_s = hidden_dim_s\n",
    "        self.hidden_dim_t = hidden_dim_t\n",
    "        self.hidden_dim_att = hidden_dim_att\n",
    "        self.do_att = do_att # should attention be used?\n",
    "        self.padding_token = padding_token\n",
    "        self.oov_token = oov_token\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.max_size = max_size\n",
    "        \n",
    "        self.max_source_idx = max(list(vocab_s.values()))\n",
    "        print('max source index',self.max_source_idx)\n",
    "        print('source vocab size',len(vocab_s))\n",
    "        \n",
    "        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n",
    "        print('max target index',self.max_target_idx)\n",
    "        print('target vocab size',len(vocab_t_inv))\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoder = Encoder(self.max_source_idx+1, self.embedding_dim_s, self.hidden_dim_s, self.padding_token).to(self.device)\n",
    "        self.decoder = Decoder(self.max_target_idx+1, self.embedding_dim_t, self.hidden_dim_s, self.hidden_dim_t, self.padding_token).to(self.device)\n",
    "        \n",
    "        if self.do_att:\n",
    "            self.att_mech = seq2seqAtt(self.hidden_dim_att, self.hidden_dim_s, self.hidden_dim_t).to(self.device)\n",
    "    \n",
    "    def my_pad(self, my_list):\n",
    "        '''my_list is a list of tuples of the form [(tensor_s_1, tensor_t_1), ..., (tensor_s_batch, tensor_t_batch)]\n",
    "        the <eos> token is appended to each sequence before padding\n",
    "        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n",
    "        batch_source = pad_sequence([torch.cat((elt[0], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
    "        batch_target = pad_sequence([torch.cat((elt[1], torch.LongTensor([self.eos_token]))) for elt in my_list], batch_first=True, padding_value=self.padding_token)\n",
    "        return batch_source, batch_target\n",
    "    \n",
    "    def forward(self, input, max_size, is_prod):\n",
    "        if is_prod: \n",
    "            input = input.unsqueeze(1) # (seq) -> (seq, 1) 1D input <=> we receive just one sentence as input (predict/production mode)\n",
    "        current_batch_size = input.size(1)\n",
    "        # fill the gap #\n",
    "        # use the encoder\n",
    "        source_hs = self.encoder(input)\n",
    "        # = = = decoder part (one timestep at a time)  = = =\n",
    "        target_h = torch.zeros(size=(1, current_batch_size, self.hidden_dim_t)).to(self.device) # init (1, batch, feat)\n",
    "        \n",
    "        # fill the gap #\n",
    "        # (initialize target_input with the proper token)\n",
    "        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1, batch)\n",
    "        pos = 0\n",
    "        eos_counter = 0\n",
    "        logits = []\n",
    "        alignements = []\n",
    "\n",
    "        while True:\n",
    "            if self.do_att:\n",
    "                source_context, norm_scores = self.att_mech(target_h, source_hs) # (1, batch, feat)\n",
    "            else:\n",
    "                source_context = source_hs[-1, :, :].unsqueeze(0) # (1, batch, feat) last hidden state of encoder\n",
    "            # fill the gap #\n",
    "            # use the decoder\n",
    "            prediction, target_h = self.decoder(target_input, source_context, target_h)\n",
    "            logits.append(prediction) # (1, batch, vocab)\n",
    "            if is_prod:\n",
    "              alignements.append(norm_scores.detach())\n",
    "            # fill the gap #\n",
    "            # get the next input to pass the decoder\n",
    "            target_input = prediction.argmax(-1)\n",
    "            eos_counter += torch.sum(target_input==self.eos_token).item()\n",
    "            pos += 1\n",
    "            if pos >= max_size or (eos_counter == current_batch_size and is_prod):\n",
    "                break\n",
    "        to_return = torch.cat(logits, 0) # logits is a list of tensors -> (seq, batch, vocab)\n",
    "        \n",
    "        if is_prod:\n",
    "            to_return = to_return.squeeze(dim=1), torch.cat(alignements, 1) # (seq, vocab)\n",
    "        \n",
    "        return to_return\n",
    "    \n",
    "    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n",
    "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
    "        optimizer = optim.Adam(parameters, lr=lr)\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n",
    "        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "        # we pass a collate function to perform padding on the fly, within each batch\n",
    "        # this is better than truncation/padding at the dataset level\n",
    "        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size, \n",
    "                                       shuffle=True, collate_fn=self.my_pad) # returns (batch, seq)\n",
    "        test_loader = data.DataLoader(testDataset, batch_size=512,\n",
    "                                      collate_fn=self.my_pad)\n",
    "        tdqm_dict_keys = ['loss', 'test loss']\n",
    "        tdqm_dict = dict(zip(tdqm_dict_keys, [0.0, 0.0]))\n",
    "        patience_counter = 1\n",
    "        patience_loss = 99999\n",
    "        \n",
    "        for epoch in range(n_epochs): \n",
    "            with tqdm(total=len(train_loader), unit_scale=True, postfix={'loss':0.0, 'test loss':0.0},\n",
    "                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1), ncols=100) as pbar:\n",
    "                for loader_idx, loader in enumerate([train_loader, test_loader]):\n",
    "                    total_loss = 0\n",
    "                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "                    if loader_idx == 0:\n",
    "                        self.train()\n",
    "                    else:\n",
    "                        self.eval()\n",
    "                    for i, (batch_source, batch_target) in enumerate(loader):\n",
    "                        batch_source = batch_source.transpose(1, 0).to(self.device) # RNN needs (seq, batch, feat) but loader returns (batch, seq)                        \n",
    "                        batch_target = batch_target.transpose(1, 0).to(self.device) # (seq, batch)\n",
    "                        \n",
    "                        # are we using the model in production\n",
    "                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq, batch), i.e., train or test\n",
    "                        if is_prod:\n",
    "                            max_size = self.max_size\n",
    "                            self.eval()\n",
    "                        else:\n",
    "                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n",
    "                        \n",
    "                        unnormalized_logits = self.forward(batch_source, max_size, is_prod)\n",
    "                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1), batch_target.flatten())\n",
    "                        total_loss += sentence_loss.item()                        \n",
    "                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)                       \n",
    "                        pbar.set_postfix(tdqm_dict)                     \n",
    "                        if loader_idx == 0:\n",
    "                            optimizer.zero_grad() # flush gradient attributes\n",
    "                            sentence_loss.backward() # compute gradients\n",
    "                            optimizer.step() # update\n",
    "                            pbar.update(1)\n",
    "            \n",
    "            if total_loss > patience_loss:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_loss = total_loss\n",
    "                patience_counter = 1 # reset\n",
    "            \n",
    "            if patience_counter > patience:\n",
    "                break\n",
    "    \n",
    "    def sourceNl_to_ints(self, source_nl):\n",
    "        '''converts natural language source sentence into source integers'''\n",
    "        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n",
    "        source_nl_clean_tok = word_tokenize(source_nl_clean, self.source_language)\n",
    "        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n",
    "                       self.oov_token for elt in source_nl_clean_tok]\n",
    "        \n",
    "        source_ints = torch.LongTensor(source_ints).to(self.device)\n",
    "        return source_ints \n",
    "    \n",
    "    def targetInts_to_nl(self, target_ints):\n",
    "        '''converts integer target sentence into target natural language'''\n",
    "        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n",
    "                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n",
    "                else self.vocab_t_inv[elt] for elt in target_ints]\n",
    "    \n",
    "    def predict(self, source_nl):\n",
    "        source_ints = self.sourceNl_to_ints(source_nl)\n",
    "        logits, alignements = self.forward(source_ints, self.max_size, True) # (seq) -> (<=max_size, vocab)\n",
    "        target_ints = logits.argmax(-1).squeeze() # (<=max_size, 1) -> (<=max_size)\n",
    "        target_nl = self.targetInts_to_nl(target_ints.tolist())\n",
    "        return ' '.join(target_nl), alignements\n",
    "        \n",
    "    def save(self, path_to_file):\n",
    "        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n",
    "        attrs['state_dict'] = self.state_dict()\n",
    "        torch.save(attrs, path_to_file)\n",
    "    \n",
    "    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n",
    "    def load(cls, path_to_file):\n",
    "        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n",
    "        state_dict = attrs.pop('state_dict')\n",
    "        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n",
    "        new.load_state_dict(state_dict)\n",
    "        return new        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B5RprtnBK-ia"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgkVw6lVUIT3"
   },
   "source": [
    "## Prepare the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "datl5SFtJ9Br",
    "outputId": "fb785ea3-e199-46e6-8a85-04e39052db32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-22 22:40:31--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\n",
      "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
      "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://vgtdqw.am.files.1drv.com/y4m3CNSvJkEyMQtd7mvFhm7V2r69_ydREm_SLzCxJbG3lILg7y9W30OQP3qS7LbWt9SDO3rlltaQo2DObybaujFG8F_UDExPPxnRBZfiHg6N1lon_XzzSiy9-7JllhcuryMKLfnsPiozDoQSUIhn4gyEIP8BaegLTMJEHC6h9beLQhr9MtMaZMxYKanTZXehgjXla0stF6YiezfHk-yr494ng/data.zip?download&psid=1 [following]\n",
      "--2021-11-22 22:40:32--  https://vgtdqw.am.files.1drv.com/y4m3CNSvJkEyMQtd7mvFhm7V2r69_ydREm_SLzCxJbG3lILg7y9W30OQP3qS7LbWt9SDO3rlltaQo2DObybaujFG8F_UDExPPxnRBZfiHg6N1lon_XzzSiy9-7JllhcuryMKLfnsPiozDoQSUIhn4gyEIP8BaegLTMJEHC6h9beLQhr9MtMaZMxYKanTZXehgjXla0stF6YiezfHk-yr494ng/data.zip?download&psid=1\n",
      "Resolving vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)... 13.107.42.12\n",
      "Connecting to vgtdqw.am.files.1drv.com (vgtdqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8994805 (8.6M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]   8.58M  4.09MB/s    in 2.1s    \n",
      "\n",
      "2021-11-22 22:40:35 (4.09 MB/s) - ‘data.zip’ saved [8994805/8994805]\n",
      "\n",
      "--2021-11-22 22:40:35--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\n",
      "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
      "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://vgtcqw.am.files.1drv.com/y4mYkBa49mAQzDLdoCxycXidK07qdNsq2j15awgUGFfdyL6VVLT1UhQlpQQMkUEYtQI6JWmKMzfJ5BhNaZ2a0SbrsOSzO0nZmEsBh7UlVYtC5D5DIPMOVly9J5qY6TAr0JtdO1T0JuA1EH5EsqVf-c6sN5NXKnOQrwJ8UDVKpvZ2Gg1oRf79rYzuoCCPiL39unPzT7ohH0F61P7lJt-w5MZTw/pretrained_moodle.pt?download&psid=1 [following]\n",
      "--2021-11-22 22:40:36--  https://vgtcqw.am.files.1drv.com/y4mYkBa49mAQzDLdoCxycXidK07qdNsq2j15awgUGFfdyL6VVLT1UhQlpQQMkUEYtQI6JWmKMzfJ5BhNaZ2a0SbrsOSzO0nZmEsBh7UlVYtC5D5DIPMOVly9J5qY6TAr0JtdO1T0JuA1EH5EsqVf-c6sN5NXKnOQrwJ8UDVKpvZ2Gg1oRf79rYzuoCCPiL39unPzT7ohH0F61P7lJt-w5MZTw/pretrained_moodle.pt?download&psid=1\n",
      "Resolving vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)... 13.107.42.12\n",
      "Connecting to vgtcqw.am.files.1drv.com (vgtcqw.am.files.1drv.com)|13.107.42.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3284775 (3.1M) [application/octet-stream]\n",
      "Saving to: ‘pretrained_moodle.pt’\n",
      "\n",
      "pretrained_moodle.p 100%[===================>]   3.13M  1.65MB/s    in 1.9s    \n",
      "\n",
      "2021-11-22 22:40:39 (1.65 MB/s) - ‘pretrained_moodle.pt’ saved [3284775/3284775]\n",
      "\n",
      "Archive:  data.zip\n",
      " extracting: pairs_test_ints.txt     \n",
      " extracting: pairs_train_ints.txt    \n",
      " extracting: README.txt              \n",
      " extracting: vocab_source.json       \n",
      " extracting: vocab_target.json       \n"
     ]
    }
   ],
   "source": [
    "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199291&authkey=AMIEuRcvDQWgoZo\" -O \"data.zip\"\n",
    "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199292&authkey=ANLtZTfpmk6tcE0\" -O \"pretrained_moodle.pt\"\n",
    "!unzip data.zip\n",
    "\n",
    "path_to_data = './'\n",
    "path_to_save_models = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wZCiFl61LPQj"
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.pairs) # total nb of observations\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "        source, target = self.pairs[idx] # one observation\n",
    "        return torch.LongTensor(source), torch.LongTensor(target)\n",
    "\n",
    "def load_pairs(train_or_test):\n",
    "    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n",
    "        pairs_tmp = file.read().splitlines()\n",
    "    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n",
    "    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n",
    "                  elt[1].split()]] for elt in pairs_tmp]\n",
    "    return pairs_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCsAk4ILTkEc"
   },
   "source": [
    "## Training / Task 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSZ-cvSuLQVt",
    "outputId": "b446e9e2-1ef2-4841-d650-77192f998445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "data prepared\n",
      "= = = attention-based model?: True = = =\n",
      "max source index 5281\n",
      "source vocab size 5278\n",
      "max target index 7459\n",
      "target vocab size 7456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch : 0/19: 100%|█████████████████| 2.13k/2.13k [03:12<00:00, 11.1it/s, loss=8.79, test loss=8.78]\n",
      "Epoch : 1/19: 100%|█████████████████| 2.13k/2.13k [03:11<00:00, 11.2it/s, loss=8.78, test loss=8.78]\n",
      "Epoch : 2/19: 100%|█████████████████| 2.13k/2.13k [03:11<00:00, 11.2it/s, loss=8.78, test loss=8.77]\n",
      "Epoch : 3/19: 100%|█████████████████| 2.13k/2.13k [03:12<00:00, 11.1it/s, loss=8.75, test loss=8.74]\n",
      "Epoch : 4/19: 100%|█████████████████| 2.13k/2.13k [03:13<00:00, 11.0it/s, loss=8.73, test loss=8.72]\n",
      "Epoch : 5/19: 100%|█████████████████| 2.13k/2.13k [03:13<00:00, 11.0it/s, loss=8.72, test loss=8.72]\n",
      "Epoch : 6/19: 100%|█████████████████| 2.13k/2.13k [03:12<00:00, 11.1it/s, loss=8.72, test loss=8.72]\n",
      "Epoch : 7/19: 100%|█████████████████| 2.13k/2.13k [03:12<00:00, 11.1it/s, loss=8.71, test loss=8.71]\n",
      "Epoch : 8/19: 100%|███████████████████| 2.13k/2.13k [03:12<00:00, 11.1it/s, loss=8.7, test loss=8.7]\n",
      "Epoch : 9/19: 100%|███████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.7, test loss=8.7]\n",
      "Epoch : 10/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.69, test loss=8.69]\n",
      "Epoch : 11/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.69, test loss=8.69]\n",
      "Epoch : 12/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.68, test loss=8.69]\n",
      "Epoch : 13/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.68, test loss=8.69]\n",
      "Epoch : 14/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.68, test loss=8.68]\n",
      "Epoch : 15/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.68, test loss=8.69]\n",
      "Epoch : 16/19: 100%|████████████████| 2.13k/2.13k [03:10<00:00, 11.2it/s, loss=8.68, test loss=8.69]\n"
     ]
    }
   ],
   "source": [
    "do_att = True # should always be set to True\n",
    "is_prod = False # production mode or not\n",
    "\n",
    "if not is_prod:\n",
    "        \n",
    "    pairs_train = load_pairs('train')\n",
    "    pairs_test = load_pairs('test')\n",
    "    \n",
    "    with open(path_to_data + 'vocab_source.json','r') as file:\n",
    "        vocab_source = json.load(file) # word -> index\n",
    "    \n",
    "    with open(path_to_data + 'vocab_target.json','r') as file:\n",
    "        vocab_target = json.load(file) # word -> index\n",
    "    \n",
    "    vocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\n",
    "    \n",
    "    print('data loaded')\n",
    "        \n",
    "    training_set = Dataset(pairs_train)\n",
    "    test_set = Dataset(pairs_test)\n",
    "    \n",
    "    print('data prepared')\n",
    "    \n",
    "    print('= = = attention-based model?:',str(do_att),'= = =')\n",
    "    \n",
    "    model = seq2seqModel(vocab_s=vocab_source,\n",
    "                         source_language='english',\n",
    "                         vocab_t_inv=vocab_target_inv,\n",
    "                         embedding_dim_s=40,\n",
    "                         embedding_dim_t=41,\n",
    "                         hidden_dim_s=30,\n",
    "                         hidden_dim_t=31,\n",
    "                         hidden_dim_att=20,\n",
    "                         do_att=do_att,\n",
    "                         padding_token=0,\n",
    "                         oov_token=1,\n",
    "                         sos_token=2,\n",
    "                         eos_token=3,\n",
    "                         max_size=30) # max size of generated sentence in prediction mode\n",
    "    \n",
    "    model.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\n",
    "    model.save(path_to_save_models + 'my_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pf0rN4RPToom"
   },
   "source": [
    "## Testing / Task 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMN8E8vZxexV",
    "outputId": "1fe8c859-d095-464b-9f78-d02aba8ac0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhXbQjP_YrgY",
    "outputId": "50c38a2d-dbd1-46ae-f09c-698f04a9477d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max source index 5281\n",
      "source vocab size 5278\n",
      "max target index 7459\n",
      "target vocab size 7456\n",
      "= = = = = \n",
      " I am a student. -> je suis étudiant . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " I have a red car. -> j ai une voiture rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " I love playing video games. -> j adore jouer à jeux jeux jeux vidéo . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " This river is full of fish. -> cette rivière est pleine de poisson . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " The fridge is full of food. -> le frigo est plein de nourriture . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " The cat fell asleep on the mat. -> le chat s est endormi sur le tapis . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " my brother likes pizza. -> mon frère aime la pizza . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " I did not mean to hurt you. -> je n ai pas voulu intention de te blesser . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " She is so mean. -> elle est tellement méchant . <EOS>\n",
      "= = = = = \n",
      " Help me pick out a tie to go with this suit! -> aidez moi à chercher une cravate pour aller avec ceci ! ! ! ! ! ! ! ! ! ! ! ! ! ! <EOS>\n",
      "= = = = = \n",
      " I can't help but smoking weed. -> je ne peux pas empêcher de de fumer fumer . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " The kids were playing hide and seek. -> les enfants jouent cache cache cache . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "= = = = = \n",
      " The cat fell asleep in front of the fireplace. -> le chat s est en du la . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "is_prod = True # production mode or not\n",
    "\n",
    "if is_prod:\n",
    "    model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
    "    # model = seq2seqModel.load(path_to_save_models + 'my_model.pt')\n",
    "    \n",
    "    to_test = ['I am a student.',\n",
    "               'I have a red car.',  # inversion captured\n",
    "               'I love playing video games.',\n",
    "                'This river is full of fish.', # plein vs pleine (accord)\n",
    "                'The fridge is full of food.',\n",
    "               'The cat fell asleep on the mat.',\n",
    "               'my brother likes pizza.', # pizza is translated to 'la pizza'\n",
    "               'I did not mean to hurt you.', # translation of mean in context\n",
    "               'She is so mean.',\n",
    "               'Help me pick out a tie to go with this suit!', # right translation\n",
    "               \"I can't help but smoking weed.\", # this one and below: hallucination\n",
    "               'The kids were playing hide and seek.',\n",
    "               'The cat fell asleep in front of the fireplace.']\n",
    "    \n",
    "    for elt in to_test:\n",
    "        translation, alignements = model.predict(elt)\n",
    "        print('= = = = = \\n','%s -> %s' % (elt, translation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUmsI29nfkFZ"
   },
   "source": [
    "## Code for question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YTFoGL14EpxH",
    "outputId": "3d301043-9946-4b89-fa68-6d0edc88582c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max source index 5281\n",
      "source vocab size 5278\n",
      "max target index 7459\n",
      "target vocab size 7456\n",
      "= = = = = \n",
      " This red car is new. -> cette voiture rouge est nouveau . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = \n",
      " I have a red bike. -> j ai un vélo rouge . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= = = = = \n",
      " The computer does not work. -> l ordinateur ne fonctionne pas . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\n",
    "# model = seq2seqModel.load(path_to_save_models + 'my_model.pt')\n",
    "\n",
    "to_test = [\"This red car is new.\",\n",
    "           \"I have a red bike.\",\n",
    "           \"The computer does not work.\"]\n",
    "\n",
    "for elt in to_test:\n",
    "    translation, alignements = model.predict(elt)\n",
    "    print('= = = = = \\n','%s -> %s' % (elt, translation))\n",
    "    plt.imshow(alignements.cpu().numpy()[:,:-24], cmap='Greys_r')\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(range(len(translation.split()[:-24])))\n",
    "    ax.set_yticks(range(len(elt.split())))\n",
    "    ax.set_xticklabels(translation.split(), rotation=45)\n",
    "    ax.set_yticklabels(elt.split())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5O8nUXDpTDS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Altegrad_2021_Lab2_NMT_Handout.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
